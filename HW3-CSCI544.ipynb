{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1677310525333,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "pBh8oTc-YAYH",
    "outputId": "8849a446-49eb-4b05-b312-8477c244bd26"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1705,
     "status": "ok",
     "timestamp": 1677310538788,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "T5WgmexEX-Es",
    "outputId": "d36b0864-ccf8-4ee5-fb55-a0d3937c8d21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/amy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/amy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/amy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/amy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# python version: 3.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z6Lxn4zX-Ex"
   },
   "source": [
    "# 1.Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcvDIZfBX-Ez"
   },
   "source": [
    "## Read Data and keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 29641,
     "status": "ok",
     "timestamp": 1677310572360,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "IDzMV7blX-Ez"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon_reviews_us_Beauty_v1_00.tsv\", sep = \"\\t\",\n",
    "                 usecols = ['star_rating', 'review_body'], \n",
    "                 on_bad_lines='skip', low_memory=False)\n",
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW0t1UVxX-E0"
   },
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3801,
     "status": "ok",
     "timestamp": 1677310580776,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "4fru4RW0X-E0"
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    if(x > \"5\"):\n",
    "        return \"-1\"\n",
    "    elif (x >= \"4\" and x <= \"5\"):\n",
    "        return \"3\"\n",
    "    elif x == \"3\":\n",
    "        return \"2\"\n",
    "    elif (x >= \"1\" and x <= \"2\"):\n",
    "        return \"1\"\n",
    "\n",
    "df['label'] = df['star_rating'].apply(lambda x : fun(x))\n",
    "class1 = df[df.label == \"1\"].sample(n = 20000, random_state = 1)\n",
    "class2 = df[df.label == \"2\"].sample(n = 20000, random_state = 1)\n",
    "class3 = df[df.label == \"3\"].sample(n = 20000, random_state = 1)\n",
    "dataset = pd.concat([class1, class2, class3], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9-k3Y47X-E1"
   },
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI0julQTX-E2"
   },
   "source": [
    "remove the HTML and URLs from the reviews since most of them contain useless word and we should remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zD80feaTX-E3"
   },
   "outputs": [],
   "source": [
    "#replace HTML of a text\n",
    "dataset['review_body'] = dataset['review_body'].str.replace('</?\\w+[^>]*>', ' ')\n",
    "#replace URL of a text\n",
    "dataset['review_body'] = dataset['review_body'].str.replace(\n",
    "    'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "            ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN4PJ1bEX-E5"
   },
   "source": [
    "# Split dataset into 80% training dataset and 20% testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1677311100147,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "HlEdqUYLX-E6"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "y = dataset['label']\n",
    "y = y.apply(lambda x : int(x))\n",
    "x = dataset['review_body']\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, \n",
    "test_size = 0.2, random_state = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uubuwjmbX-E6"
   },
   "source": [
    "# 2. Word Embedding\n",
    "## reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8wfEsSfX-E6"
   },
   "source": [
    "## （a）Load the pretrained “word2vec-google-news-300” Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 54465,
     "status": "ok",
     "timestamp": 1677311159540,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "8ZfNKoe1X-E7"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1677311242470,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "6a_vK1nKX-E7",
    "outputId": "6edec9b1-1ffc-475b-8884-a45100751cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'excellent'\t'outstanding'\t0.56\n",
      "'man'\t'king'\t0.23\n",
      "'woman'\t'queen'\t0.32\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('excellent', 'outstanding'),  \n",
    "    ('man', 'king'),  \n",
    "    ('woman', 'queen'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taKiNiNuX-E7"
   },
   "source": [
    "## (b) Train a Word2Vec model using my own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, generate corpus based on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3582,
     "status": "ok",
     "timestamp": 1677311247703,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "Jq62lMTnX-E7"
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "myCorpus = []\n",
    "for d in dataset['review_body']:\n",
    "    myCorpus.append(utils.simple_preprocess(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 16604,
     "status": "ok",
     "timestamp": 1677311291314,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "09ZHr2QYX-E7"
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "sentences = myCorpus\n",
    "myModel = gensim.models.Word2Vec(sentences=sentences,vector_size=300, window=13, min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1677311295042,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "eB85IU-zX-E8",
    "outputId": "be18147d-4e63-4684-e5cd-a64af60fa860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'excellent'\t'outstanding'\t0.66\n",
      "'man'\t'king'\t0.36\n",
      "'woman'\t'queen'\t0.29\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('excellent', 'outstanding'),  \n",
    "    ('man', 'king'),  \n",
    "    ('woman', 'queen'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, myModel.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  It seems the model I trained based on our dataset has better encoding capability since the semantic similarity between words pair in our example has higher scores. I have learned that the corpus word2vec-google-news-300 used is much larger than our dataset. Therefore, different corpus or different size between two corpus may give model different parameters and output different scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzl-Ah0VX-E8"
   },
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrLqtxbzX-E8"
   },
   "source": [
    "## First, generate the average Word2Vec vectors for each review as the input feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1677312165562,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "bTrit72SX-E8"
   },
   "outputs": [],
   "source": [
    "def getAverageWV(dataset, wv):\n",
    "    averageWV = []\n",
    "    for review in dataset:\n",
    "        count = 0\n",
    "        reviewVec = np.zeros(300)\n",
    "        words = review.split()\n",
    "        for word in words:\n",
    "            if word in wv:\n",
    "                count = count + 1 \n",
    "                reviewVec = reviewVec + wv[word]\n",
    "        if count > 0:\n",
    "            averageWV.append(reviewVec / count)\n",
    "        else:\n",
    "            averageWV.append(np.zeros(300))\n",
    "    return averageWV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 12368,
     "status": "ok",
     "timestamp": 1677312192152,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "Tn0lOMqIX-E9"
   },
   "outputs": [],
   "source": [
    "x_vec_train = getAverageWV(x_train, wv)\n",
    "y_vec_train = y_train\n",
    "x_vec_test = getAverageWV(x_test, wv)\n",
    "y_vec_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1677312205985,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "R_yNizJtOPiu"
   },
   "outputs": [],
   "source": [
    "y_vec_train = y_vec_train.apply(lambda x : (x - 1))\n",
    "y_vec_test = y_vec_test.apply(lambda x : (x - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ9wJhStX-E9"
   },
   "source": [
    "## Perceptron\n",
    "### here I use the same perceptron model as HW1 does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4663,
     "status": "ok",
     "timestamp": 1677311488062,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "RRHuM0HBX-E9"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron(max_iter = 80, tol = 1e-5, eta0 = 0.01, penalty = 'elasticnet')\n",
    "perceptron.fit(x_vec_train, y_vec_train)\n",
    "y_vec_test_pred = perceptron.predict(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1677311513629,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "848cYsL6biEP",
    "outputId": "0ec61003-f0c2-4593-8021-18fddba8d6ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5753333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_vec_test, y_vec_test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : The perceptron accuracy based on TF-IDF features is : 0.6246666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZH_ZjxqX-E-"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 118357,
     "status": "ok",
     "timestamp": 1677311636345,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "Tgf-sBsfX-E-"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(penalty = 'l2', loss = 'squared_hinge', multi_class= 'ovr',\n",
    "                intercept_scaling=1, max_iter=1000)\n",
    "svm.fit(x_vec_train, y_vec_train)\n",
    "y_vec_test_pred = svm.predict(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1677311640671,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "LclaHrsocvXc",
    "outputId": "2f701880-d273-47e8-993f-9ff6d507cdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6300833333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_vec_test, y_vec_test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : The SVM accuracy based on TF-IDF features is : 0.694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Both perceptron and SVM model based on TF-IDF features get higher accuracy. It gives us a direction that the extracted TF-IDF features perform better on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgBIXPMGX-E-"
   },
   "source": [
    "# 4. Feedforward Neural Networks\n",
    "### (a) Using the Word2Vec features generated from Task3, train a feedforward multilayer perceptron network for classification\n",
    "#### Reference: https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input size = 300, \n",
    "hidden1_size = 100, \n",
    "hidden2_size = 10, \n",
    "output_size = 3 -> 3-classfication problem, label is 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1677311652365,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "aaxVGsN3X-E_",
    "outputId": "ceacc689-fb06-49e2-e9d4-5689bd122748"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Word2Vec features generated from Task3.  \n",
    "Here I convert x_vec_train and x_vec_test into float type and then convert them into tensor. For y_vec_train and y_vec_test, I convert them into float type and then Long type for future computing and then convert them into tensor. The tensor type is what I needed to feed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1677311657552,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "hWzm2wDB6Qk2"
   },
   "outputs": [],
   "source": [
    "x_FNN_train = torch.tensor(np.asarray(x_vec_train), dtype = torch.float32)\n",
    "x_FNN_test = torch.tensor(np.asarray(x_vec_test), dtype = torch.float32)\n",
    "y_FNN_train = torch.tensor(np.asarray(y_vec_train), dtype = torch.float32)\n",
    "y_FNN_test = torch.tensor(np.asarray(y_vec_test), dtype = torch.float32)\n",
    "y_FNN_train = y_FNN_train.type(torch.LongTensor)\n",
    "y_FNN_test = y_FNN_test.type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 2347,
     "status": "error",
     "timestamp": 1677312250296,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "_X5atZcvX-E_",
    "outputId": "9016166e-8149-43e7-9afb-7d87c56def14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Traning Loss: 0.977401\n",
      "Epoch: 20 \t Traning Loss: 0.911542\n",
      "Epoch: 30 \t Traning Loss: 0.866037\n",
      "Epoch: 40 \t Traning Loss: 0.837069\n",
      "Epoch: 50 \t Traning Loss: 0.814971\n",
      "Epoch: 60 \t Traning Loss: 0.796916\n",
      "Epoch: 70 \t Traning Loss: 0.790416\n",
      "Epoch: 80 \t Traning Loss: 0.773514\n",
      "Epoch: 90 \t Traning Loss: 0.761992\n",
      "Epoch: 100 \t Traning Loss: 0.751186\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_FNN_train)\n",
    "    loss = criterion(output, y_FNN_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(epoch % 10 == 0):\n",
    "        print('Epoch: {} \\t Traning Loss: {:.6f}'.format(epoch, loss.item()))     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1677311797345,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "sDSJ0k-GX-E_",
    "outputId": "af61be3e-d815-4b2f-c554-15ac6f326a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.6405\n"
     ]
    }
   ],
   "source": [
    "y_FNN_test_pred = model(x_FNN_test)\n",
    "y_FNN_test_pred = y_FNN_test_pred.detach().numpy().argmax(axis = 1)\n",
    "\n",
    "totalCount = 0\n",
    "currentCount = 0\n",
    "for i in range(len(y_FNN_test_pred)):\n",
    "    totalCount = totalCount + 1\n",
    "    if y_FNN_test_pred[i] == y_FNN_test[i]:\n",
    "        currentCount = currentCount + 1\n",
    "\n",
    "if(totalCount > 0):\n",
    "    print('Accuracy is : ', currentCount / totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJia8pRaX-E_"
   },
   "source": [
    "#### (b) To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature\n",
    "We deal with word by word in each review. We skip a word if it is not in wv. We add the wv score of a word if it is in wv until the current size of reviewVec is equal to 10 or this review has no word to read. If size of reviewVec is less than 10, we append np.zeros(300) into reviewVec to make its size equal to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1677311808233,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "ndfby7wqX-FA"
   },
   "outputs": [],
   "source": [
    "def getAverageWV_10(dataset, wv):\n",
    "    averageWV = []\n",
    "    for review in dataset:\n",
    "        count = 10\n",
    "        reviewVec = []\n",
    "        words = review.split()\n",
    "        for word in words:\n",
    "            if count > 0 and word in wv:\n",
    "                reviewVec.append(wv[word])\n",
    "                count = count - 1\n",
    "        while count > 0:\n",
    "            reviewVec.append(np.zeros(300))\n",
    "            count = count - 1\n",
    "        averageWV.append(reviewVec)\n",
    "    return averageWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1982,
     "status": "ok",
     "timestamp": 1677311812027,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "VwvJCJepX-FA"
   },
   "outputs": [],
   "source": [
    "x_vec_10_train = getAverageWV_10(x_train, wv)\n",
    "y_vec_10_train = y_train\n",
    "x_vec_10_test = getAverageWV_10(x_test, wv)\n",
    "y_vec_10_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1677311813622,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "hUz2B7HzR4w-"
   },
   "outputs": [],
   "source": [
    "y_vec_10_train = y_vec_10_train.apply(lambda x : (x - 1))\n",
    "y_vec_10_test = y_vec_10_test.apply(lambda x : (x - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Word2Vec features generated from Task3.  \n",
    "Here I convert x_vec_10_train and x_vec_10_test into float type and then convert them into tensor. For y_vec_10_train and y_vec_10_test, I convert them into float type and then Long type for future computing and then convert them into tensor. The tensor type is what I needed to feed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1677311816583,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "QXIzBjM5CR_u"
   },
   "outputs": [],
   "source": [
    "x_FNN_10_train = torch.tensor(np.asarray(x_vec_10_train), dtype = torch.float32)\n",
    "x_FNN_10_test = torch.tensor(np.asarray(x_vec_10_test), dtype = torch.float32)\n",
    "y_FNN_10_train = torch.tensor(np.asarray(y_vec_10_train), dtype = torch.float32)\n",
    "y_FNN_10_test = torch.tensor(np.asarray(y_vec_10_test), dtype = torch.float32)\n",
    "y_FNN_10_train = y_FNN_10_train.type(torch.LongTensor)\n",
    "y_FNN_10_test = y_FNN_10_test.type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the input_size is 300 * 10 as first 10 Word2Vec are collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden1_size = 100, hidden2_size = 10, output_size = 3 -> 3-classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1677311819161,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "JOTxYAT8Mrtl",
    "outputId": "90530077-b632-4f95-e747-0bf4490acbc2"
   },
   "outputs": [],
   "source": [
    "class ConcatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(300 * 10, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten input into 300*10 size to feed into model\n",
    "        x = x.view(-1, 300 * 10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "concatFNNModel = ConcatNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6329,
     "status": "ok",
     "timestamp": 1677311828699,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "IVjA4Kz1X-FA",
    "outputId": "7fb9e2d3-58e0-4791-d692-f4877d786f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Traning Loss: 1.000331\n",
      "Epoch: 20 \t Traning Loss: 0.914064\n",
      "Epoch: 30 \t Traning Loss: 0.874733\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(concatFNNModel.parameters(), lr=0.01)\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    concatFNNModel.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = concatFNNModel(x_FNN_10_train)\n",
    "    loss = criterion(output, y_FNN_10_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {} \\t Traning Loss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1677311830645,
     "user": {
      "displayName": "Yimei Ma",
      "userId": "13233983422453319548"
     },
     "user_tz": 480
    },
    "id": "swE6hdLFX-FA",
    "outputId": "d7ffc570-be41-4426-c8bc-4c4cc2d930fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 1 0 2]\n",
      "Accuracy is :  0.5529166666666666\n"
     ]
    }
   ],
   "source": [
    "y_FNN_10_test_pred = concatFNNModel(x_FNN_10_test)\n",
    "y_FNN_10_test_pred = y_FNN_10_test_pred.detach().numpy().argmax(axis = 1)\n",
    "print(y_FNN_10_test_pred)\n",
    "\n",
    "totalCount = 0\n",
    "currentCount = 0\n",
    "for i in range(len(y_FNN_10_test_pred)):\n",
    "    totalCount = totalCount + 1\n",
    "    if y_FNN_10_test_pred[i] == y_FNN_10_test[i]:\n",
    "        currentCount = currentCount + 1\n",
    "\n",
    "if(totalCount > 0):\n",
    "    print('Accuracy is : ', currentCount / totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It seems FNN based on average Word2Vec has higher accuracy.I have checked our dataset, the average of each review is more than 10 words, which means first 10 Word2Vec may not be enough for this 3-classification problem. However, due to smaller size of first 10 Word2Vec, the computing speed of it increases a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR2IGPiIX-FB"
   },
   "source": [
    "# 5. Recurrent Neural Networks\n",
    "### Reference: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To generate the input features, concatenate the first 20 Word2Vec vectors for each review as the input feature\n",
    "We deal with word by word in each review. We skip a word if it is not in wv. We add the wv score of a word if it is in wv until the current size of reviewVec is equal to 20 or this review has no word to read. If size of reviewVec is less than 20, we append np.zeros(300) into reviewVec to make its size equal to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageWV_20(dataset, wv):\n",
    "    averageWV = []\n",
    "    for review in dataset:\n",
    "        count = 20\n",
    "        reviewVec = []\n",
    "        words = review.split()\n",
    "        for word in words:\n",
    "            if count > 0 and word in wv:\n",
    "                reviewVec.append(wv[word])\n",
    "                count = count - 1 \n",
    "        while count > 0:\n",
    "            reviewVec.append(np.zeros(300))\n",
    "            count = count - 1\n",
    "        averageWV.append(reviewVec)\n",
    "    return averageWV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function for DataLoader since we need to split our dataset into specific batch size and get (padded_data, label, length) to feed into model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    X = []\n",
    "    label = []\n",
    "    length = []\n",
    "    for x in batch:\n",
    "        x_vec_tensor = torch.tensor(x[0], dtype=torch.float32)\n",
    "        X.append(x_vec_tensor)\n",
    "        label.append(x[1])\n",
    "        length.append(len(x[0]))\n",
    "    res = pad_sequence(X, batch_first = True, padding_value = 0)\n",
    "    return res, label, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec_20_train = getAverageWV_20(x_train, wv)\n",
    "y_vec_20_train = y_train\n",
    "x_vec_20_test = getAverageWV_20(x_test, wv)\n",
    "y_vec_20_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vec_20_train = y_vec_20_train.apply(lambda x : (x - 1))\n",
    "y_vec_20_test = y_vec_20_test.apply(lambda x : (x - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I convert x_vec_20_train and x_vec_20_test into float type and then convert them into tensor. For y_vec_20_train and y_vec_20_test, I convert them into float type and then Long type for future computing and then convert them into tensor. The tensor type is what I needed to feed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_RNN_20_train = torch.tensor(np.asarray(x_vec_20_train), dtype = torch.float32)\n",
    "x_RNN_20_test = torch.tensor(np.asarray(x_vec_20_test), dtype = torch.float32)\n",
    "y_RNN_20_train = torch.tensor(np.asarray(y_vec_20_train), dtype = torch.float32)\n",
    "y_RNN_20_test = torch.tensor(np.asarray(y_vec_20_test), dtype = torch.float32)\n",
    "y_RNN_20_train = y_RNN_20_train.type(torch.LongTensor)\n",
    "y_RNN_20_test = y_RNN_20_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider an RNN cell with the hidden state size of 20 for this 3-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, in order to perform batch processing more efficiently, it is necessary to fill the sample sequence to ensure that each sample has the same length. Although the length of the filled sample sequence is the same, the sequence may be filled with a lot of invalid value 0. Feeding the filled value 0 to the RNN for forward calculation will not only waste computing resources, but may also have errors in the final value. Therefore, before sending the sequence to RNN for processing, pack_padded_sequence needs to be used for compression to compress invalid padding values. After the sequence is processed by RNN, the output is still a compressed sequence. It is necessary to use pad_packed_sequence to fill the compressed sequence back for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "num_layers = 1\n",
    "sequence_length = 20 #length of review as required\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(sequence_length * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        batch_size = input.size(0)\n",
    "        hidden = self.initHidden(batch_size)\n",
    "        x_packed = pack_padded_sequence(input, length, batch_first = True, enforce_sorted = False)\n",
    "        output, _ = self.layer(x_packed, hidden)\n",
    "        output, output_length = pad_packed_sequence(output, batch_first = True)\n",
    "        output = self.fc(output.reshape(output.shape[0], -1))\n",
    "        return output\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Traning Loss: 0.942759\n",
      "Epoch: 2 \t Traning Loss: 0.911457\n",
      "Epoch: 3 \t Traning Loss: 0.905028\n",
      "Epoch: 4 \t Traning Loss: 0.879520\n",
      "Epoch: 5 \t Traning Loss: 0.865990\n",
      "Epoch: 6 \t Traning Loss: 0.862684\n",
      "Epoch: 7 \t Traning Loss: 0.860867\n",
      "Epoch: 8 \t Traning Loss: 0.859361\n",
      "Epoch: 9 \t Traning Loss: 0.857098\n",
      "Epoch: 10 \t Traning Loss: 0.852774\n",
      "Epoch: 11 \t Traning Loss: 0.846582\n",
      "Epoch: 12 \t Traning Loss: 0.840210\n",
      "Epoch: 13 \t Traning Loss: 0.834388\n",
      "Epoch: 14 \t Traning Loss: 0.828894\n",
      "Epoch: 15 \t Traning Loss: 0.823316\n",
      "Epoch: 16 \t Traning Loss: 0.817630\n",
      "Epoch: 17 \t Traning Loss: 0.812294\n",
      "Epoch: 18 \t Traning Loss: 0.807734\n",
      "Epoch: 19 \t Traning Loss: 0.803968\n",
      "Epoch: 20 \t Traning Loss: 0.801633\n",
      "Epoch: 21 \t Traning Loss: 0.800215\n",
      "Epoch: 22 \t Traning Loss: 0.799657\n",
      "Epoch: 23 \t Traning Loss: 0.799701\n",
      "Epoch: 24 \t Traning Loss: 0.799466\n",
      "Epoch: 25 \t Traning Loss: 0.799051\n",
      "Epoch: 26 \t Traning Loss: 0.798524\n",
      "Epoch: 27 \t Traning Loss: 0.798798\n",
      "Epoch: 28 \t Traning Loss: 0.798449\n",
      "Epoch: 29 \t Traning Loss: 0.799427\n",
      "Epoch: 30 \t Traning Loss: 0.801444\n",
      "Epoch: 31 \t Traning Loss: 0.802842\n",
      "Epoch: 32 \t Traning Loss: 0.802983\n",
      "Epoch: 33 \t Traning Loss: 0.801540\n",
      "Epoch: 34 \t Traning Loss: 0.804390\n",
      "Epoch: 35 \t Traning Loss: 0.808160\n",
      "Epoch: 36 \t Traning Loss: 0.806957\n",
      "Epoch: 37 \t Traning Loss: 0.809260\n",
      "Epoch: 38 \t Traning Loss: 0.809263\n",
      "Epoch: 39 \t Traning Loss: 0.809028\n",
      "Epoch: 40 \t Traning Loss: 0.807322\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.002)\n",
    "\n",
    "n_epochs = 40\n",
    "BATCH_SIZE = 64\n",
    "rnn_dataset = Data.TensorDataset(x_RNN_20_train,y_RNN_20_train)\n",
    "rnn_loader = Data.DataLoader(dataset = rnn_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    rnn.train()\n",
    "    for data, target,length in rnn_loader:\n",
    "        target = torch.tensor(target, dtype = torch.long)\n",
    "        output = rnn(data, length)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\t Traning Loss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_test_dataset = Data.TensorDataset(x_RNN_20_test,y_RNN_20_test)\n",
    "rnn_test_loader = Data.DataLoader(dataset = rnn_test_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is : 0.5863333333333334\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    totalCount = 0\n",
    "    currentCount = 0\n",
    "    for data, target, length in rnn_test_loader:\n",
    "        output = rnn(data, length)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        currentCount += sum(np.array(pred) == target)\n",
    "        totalCount += pred.size(0)\n",
    "    rnn.train()\n",
    "    print('accuracy is :', currentCount / totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We could learn that the accuracy of simple RNN is between that of FNN based on avaerage wv and first-10-wv FNN. FNN based on average wv still gives better accuracy since it contains average vector of each reviews, which could deliver more information to model for classification problem. And for this compared to first-10-wv FNN, Task5 leans from first-20 vectors, so it contains more information, which is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider an RNN cell with the hidden state size of 20 for this 3-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, in order to perform batch processing more efficiently, it is necessary to fill the sample sequence to ensure that each sample has the same length. Although the length of the filled sample sequence is the same, the sequence may be filled with a lot of invalid value 0. Feeding the filled value 0 to the RNN for forward calculation will not only waste computing resources, but may also have errors in the final value. Therefore, before sending the sequence to RNN for processing, pack_padded_sequence needs to be used for compression to compress invalid padding values. After the sequence is processed by RNN, the output is still a compressed sequence. It is necessary to use pad_packed_sequence to fill the compressed sequence back for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "num_layers = 1\n",
    "sequence_length = 20\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(sequence_length * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        batch_size = input.size(0)\n",
    "        hidden = self.initHidden(batch_size)\n",
    "        x_packed = pack_padded_sequence(input, length, batch_first = True, enforce_sorted = False)\n",
    "        output, _ = self.layer(x_packed, hidden)\n",
    "        output, output_length = pad_packed_sequence(output, batch_first = True)\n",
    "        output = self.fc(output.reshape(output.shape[0], -1))\n",
    "        return output\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "GRU = GRUModel(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Traning Loss: 0.954589\n",
      "Epoch: 2 \t Traning Loss: 0.933131\n",
      "Epoch: 3 \t Traning Loss: 0.936841\n",
      "Epoch: 4 \t Traning Loss: 0.921497\n",
      "Epoch: 5 \t Traning Loss: 0.862357\n",
      "Epoch: 6 \t Traning Loss: 0.805082\n",
      "Epoch: 7 \t Traning Loss: 0.741512\n",
      "Epoch: 8 \t Traning Loss: 0.674689\n",
      "Epoch: 9 \t Traning Loss: 0.656668\n",
      "Epoch: 10 \t Traning Loss: 0.680654\n",
      "Epoch: 11 \t Traning Loss: 0.554102\n",
      "Epoch: 12 \t Traning Loss: 0.597045\n",
      "Epoch: 13 \t Traning Loss: 0.556302\n",
      "Epoch: 14 \t Traning Loss: 0.546781\n",
      "Epoch: 15 \t Traning Loss: 0.534088\n",
      "Epoch: 16 \t Traning Loss: 0.481103\n",
      "Epoch: 17 \t Traning Loss: 0.521146\n",
      "Epoch: 18 \t Traning Loss: 0.488755\n",
      "Epoch: 19 \t Traning Loss: 0.515605\n",
      "Epoch: 20 \t Traning Loss: 0.485159\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(GRU.parameters(), lr=0.008)\n",
    "\n",
    "n_epochs = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "GRU_dataset = Data.TensorDataset(x_RNN_20_train,y_RNN_20_train)\n",
    "GRU_loader = Data.DataLoader(dataset = GRU_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    GRU.train()\n",
    "    for data, target,length in GRU_loader:\n",
    "        target = torch.tensor(target, dtype = torch.long)\n",
    "        output = GRU(data, length)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\t Traning Loss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_test_dataset = Data.TensorDataset(x_RNN_20_test,y_RNN_20_test)\n",
    "GRU_test_loader = Data.DataLoader(dataset = GRU_test_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is : 0.5969166666666667\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    totalCount = 0\n",
    "    currentCount = 0\n",
    "    for data, target, length in rnn_test_loader:\n",
    "        output = GRU(data, length)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        currentCount += sum(np.array(pred) == target)\n",
    "        totalCount += pred.size(0)\n",
    "    rnn.train()\n",
    "    print('accuracy is :', currentCount / totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could learn that the accuracy of GRU is between that of FNN based on avaerage wv and first-10-wv FNN. FNN based on average wv still gives better accuracy since it contains average vector of each reviews, which could deliver more information to model for classification problem. And for this compared to first-10-wv FNN, Task5 leans from first-20 vectors, so it contains more information, which is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （c）LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, in order to perform batch processing more efficiently, it is necessary to fill the sample sequence to ensure that each sample has the same length. Although the length of the filled sample sequence is the same, the sequence may be filled with a lot of invalid value 0. Feeding the filled value 0 to the RNN for forward calculation will not only waste computing resources, but may also have errors in the final value. Therefore, before sending the sequence to RNN for processing, pack_padded_sequence needs to be used for compression to compress invalid padding values. After the sequence is processed by RNN, the output is still a compressed sequence. It is necessary to use pad_packed_sequence to fill the compressed sequence back for subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to RNN and GRU, LSTM has one more cell state needed to feed into output gate, here I add one more thing in initHidden()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "num_layers = 1\n",
    "sequence_length = 20\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(sequence_length * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        batch_size = input.size(0)\n",
    "        hidden = self.initHidden(batch_size)\n",
    "        x_packed = pack_padded_sequence(input, length, batch_first = True, enforce_sorted = False)\n",
    "        output, hidden = self.layer(x_packed, hidden)\n",
    "        output, output_length = pad_packed_sequence(output, batch_first = True)\n",
    "        output = self.fc(output.reshape(output.shape[0], -1))\n",
    "        return output\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "LSTM = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Traning Loss: 0.941118\n",
      "Epoch: 2 \t Traning Loss: 0.878589\n",
      "Epoch: 3 \t Traning Loss: 0.830294\n",
      "Epoch: 4 \t Traning Loss: 0.787551\n",
      "Epoch: 5 \t Traning Loss: 0.763168\n",
      "Epoch: 6 \t Traning Loss: 0.739457\n",
      "Epoch: 7 \t Traning Loss: 0.721872\n",
      "Epoch: 8 \t Traning Loss: 0.716214\n",
      "Epoch: 9 \t Traning Loss: 0.701400\n",
      "Epoch: 10 \t Traning Loss: 0.698533\n",
      "Epoch: 11 \t Traning Loss: 0.645968\n",
      "Epoch: 12 \t Traning Loss: 0.613846\n",
      "Epoch: 13 \t Traning Loss: 0.590450\n",
      "Epoch: 14 \t Traning Loss: 0.565637\n",
      "Epoch: 15 \t Traning Loss: 0.531812\n",
      "Epoch: 16 \t Traning Loss: 0.519008\n",
      "Epoch: 17 \t Traning Loss: 0.529143\n",
      "Epoch: 18 \t Traning Loss: 0.508652\n",
      "Epoch: 19 \t Traning Loss: 0.492074\n",
      "Epoch: 20 \t Traning Loss: 0.503269\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(LSTM.parameters(), lr=0.005)\n",
    "\n",
    "n_epochs = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LSTM_dataset = Data.TensorDataset(x_RNN_20_train,y_RNN_20_train)\n",
    "LSTM_loader = Data.DataLoader(dataset = LSTM_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    LSTM.train()\n",
    "    for data, target,length in LSTM_loader:\n",
    "        target = torch.tensor(target, dtype = torch.long)\n",
    "        output = LSTM(data, length)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} \\t Traning Loss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_test_dataset = Data.TensorDataset(x_RNN_20_test,y_RNN_20_test)\n",
    "LSTM_test_loader = Data.DataLoader(dataset = LSTM_test_dataset, batch_size = BATCH_SIZE, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is : 0.6000833333333333\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    totalCount = 0\n",
    "    currentCount = 0\n",
    "    for data, target, length in LSTM_test_loader:\n",
    "        output = LSTM(data, length)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        currentCount += sum(np.array(pred) == target)\n",
    "        totalCount += pred.size(0)\n",
    "    rnn.train()\n",
    "    print('accuracy is :', currentCount / totalCount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:We could learn that the accuracy of LSTM is between that of FNN based on avaerage wv and first-10-wv FNN. FNN based on average wv still gives better accuracy since it contains average vector of each reviews, which could deliver more information to model for classification problem. And for this compared to first-10-wv FNN, Task5 leans from first-20 vectors, so it contains more information, which is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: For this part with review length equal to 20, LSTM gives me best accuracy while simple RNN gives me lowest accuracy. Since LSTM could be used to solve long-dependencies problem, which may reflect that our review with size of 20 has longer dependencies than we expected. This is also something simple RNN missed. And for GRU, its gate obviously generate better results than simple RNN, which means its gate is useful and could improve performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
